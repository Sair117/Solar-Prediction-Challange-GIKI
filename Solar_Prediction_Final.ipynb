{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT"
      ],
      "metadata": {
        "id": "ez3Xmq9MfeqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install xgboost lightgbm catboost optuna"
      ],
      "metadata": {
        "id": "3a-mm04WhefY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tPJFHx4fa8Y"
      },
      "outputs": [],
      "source": [
        "# ADVANCED TREE-BASED ENSEMBLE WITH STACKING\n",
        "# Combines LightGBM, XGBoost, and CatBoost with meta-learning\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import KFold, GroupKFold\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import optuna  # For hyperparameter optimization\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ADVANCED TREE-BASED STACKING PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# 1. LOAD AND PREPARE DATA\n",
        "# ============================================\n",
        "\n",
        "print(\"\\nüìÅ Loading data...\")\n",
        "\n",
        "train_df = pd.read_csv('/content/train_data.csv')\n",
        "test_df_original = pd.read_csv('/content/test_data_masked.csv')\n",
        "systems_df = pd.read_csv('/content/systems_new.csv')\n",
        "sample_submission = pd.read_csv('/content/sample_submissions.csv')\n",
        "\n",
        "# Convert timestamps\n",
        "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
        "test_df_original['timestamp'] = pd.to_datetime(test_df_original['timestamp'])\n",
        "\n",
        "# Fix capacity units\n",
        "systems_df['panels_capacity_W'] = systems_df['panels_capacity'] * 1000\n",
        "systems_df['load_capacity_W'] = systems_df['load_capacity'] * 1000\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(train_df):,} training samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "9BfK2-JY36Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train date range:\", train_df[\"timestamp\"].min(), \"to\", train_df[\"timestamp\"].max())\n",
        "print(\"Test date range:\", test_df_original[\"timestamp\"].min(), \"to\", test_df_original[\"timestamp\"].max())\n",
        "print(\"Train systems:\", train_df[\"system_id\"].nunique())\n",
        "print(\"Test systems:\", test_df_original[\"system_id\"].nunique())\n",
        "print(\"Metadata systems:\", systems_df[\"system_id\"].nunique())\n",
        "\n",
        "# Check alignment\n",
        "print(\"Systems missing in train:\", set(systems_df[\"system_id\"]) - set(train_df[\"system_id\"]))\n",
        "print(\"Systems missing in test:\", set(systems_df[\"system_id\"]) - set(test_df_original[\"system_id\"]))"
      ],
      "metadata": {
        "id": "Aozkk_zL35_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NaNs\n",
        "print(train_df.isna().sum())\n",
        "\n",
        "# Placeholders\n",
        "print(\"Placeholder counts (train):\")\n",
        "print((train_df[[\"generation_W\",\"load_W\"]] < 0).sum())\n",
        "\n",
        "print(\"Placeholder counts (test):\")\n",
        "print((test_df_original[[\"generation_W\",\"load_W\"]] < 0).sum())\n",
        "\n",
        "# Per system missing\n",
        "missing_summary = train_df.groupby(\"system_id\")[[\"generation_W\",\"load_W\"]].apply(lambda x: (x<=0).sum())\n",
        "print(missing_summary.head())"
      ],
      "metadata": {
        "id": "MfGS7djj4H8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_time_gaps(df, sys_id):\n",
        "    sub = df[df[\"system_id\"] == sys_id].sort_values(\"timestamp\").copy() # Add .copy() to avoid SettingWithCopyWarning\n",
        "    sub[\"timestamp\"] = pd.to_datetime(sub[\"timestamp\"]) # Convert to datetime objects\n",
        "    diffs = sub[\"timestamp\"].diff().dropna()\n",
        "    return diffs.value_counts()\n",
        "\n",
        "# Convert timestamp columns in train and test dataframes to datetime objects\n",
        "train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"])\n",
        "test_df_original[\"timestamp\"] = pd.to_datetime(test_df_original[\"timestamp\"])\n",
        "\n",
        "for sys in train_df[\"system_id\"].unique()[:3]:  # sample a few\n",
        "    print(\"System\", sys, check_time_gaps(train_df, sys).head())\n",
        "\n",
        "# Any duplicates?\n",
        "dup = train_df.duplicated(subset=[\"system_id\",\"timestamp\"]).sum()\n",
        "print(\"Duplicate rows:\", dup)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zUi1YUJ17Sdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge capacity metadata\n",
        "train_df = train_df.merge(systems_df, on=\"system_id\", how=\"left\")\n",
        "\n",
        "# Negative values\n",
        "negatives = (train_df[[\"generation_W\",\"load_W\"]] < 0).sum()\n",
        "print(\"Negative counts:\\n\", negatives)\n",
        "\n",
        "# Outliers vs capacity\n",
        "train_df[\"gen_capacity_ratio\"] = train_df[\"generation_W\"] / (train_df[\"panels_capacity\"]*1000)\n",
        "train_df[\"load_capacity_ratio\"] = train_df[\"load_W\"] / (train_df[\"load_capacity\"]*1000)\n",
        "\n",
        "print(train_df[\"gen_capacity_ratio\"].describe())\n",
        "print(train_df[\"load_capacity_ratio\"].describe())\n",
        "\n",
        "# Flag suspicious\n",
        "suspicious = train_df[(train_df[\"gen_capacity_ratio\"] > 1.5) | (train_df[\"gen_capacity_ratio\"] < 0)]\n",
        "print(\"Suspicious rows:\", suspicious.shape)"
      ],
      "metadata": {
        "id": "gHH43g6cArl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"hour\"] = train_df[\"timestamp\"].dt.hour\n",
        "train_df[\"month\"] = train_df[\"timestamp\"].dt.month\n",
        "\n",
        "hourly = train_df.groupby(\"hour\")[[\"generation_W\",\"load_W\"]].mean()\n",
        "monthly = train_df.groupby(\"month\")[[\"generation_W\",\"load_W\"]].mean()\n",
        "\n",
        "hourly.plot(title=\"Avg Generation/Load by Hour\", figsize=(10,5))\n",
        "monthly.plot(title=\"Avg Generation/Load by Month\", figsize=(10,5))"
      ],
      "metadata": {
        "id": "Vcg_IY-wAzFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "\n",
        "sns.boxplot(data=train_df, x=\"connection_type\", y=\"generation_W\")\n",
        "plt.title(\"Generation Distribution by Connection Type\")\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(data=train_df, x=\"location\", y=\"generation_W\")\n",
        "plt.title(\"Generation Distribution by City\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LT-kRBOoA8Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE ENGINEERING AND SELECTION"
      ],
      "metadata": {
        "id": "Mm1ECOyJfn47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 2. ENHANCED FEATURE ENGINEERING\n",
        "# ============================================\n",
        "\n",
        "def create_advanced_features(df, systems_df, top_locations=None):\n",
        "    \"\"\"Create more sophisticated features with consistent location encoding\"\"\"\n",
        "\n",
        "    # Merge with systems\n",
        "    df = df.merge(systems_df[['system_id', 'panels_capacity_W', 'load_capacity_W',\n",
        "                              'connection_type', 'location']],\n",
        "                  on='system_id', how='left')\n",
        "\n",
        "    # Time features\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['minute'] = df['timestamp'].dt.minute\n",
        "    df['day_of_year'] = df['timestamp'].dt.dayofyear\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "    df['quarter'] = df['timestamp'].dt.quarter\n",
        "    df['week_of_year'] = df['timestamp'].dt.isocalendar().week\n",
        "\n",
        "    # Cyclical encoding\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "    # Enhanced solar features\n",
        "    df['solar_elevation'] = np.where(\n",
        "        (df['hour'] >= 6) & (df['hour'] <= 18),\n",
        "        np.sin(np.pi * (df['hour'] - 6) / 12),\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # Solar intensity approximation (considers time and season)\n",
        "    summer_factor = np.sin(np.pi * (df['day_of_year'] - 80) / 365)  # Peak around June\n",
        "    df['solar_intensity'] = df['solar_elevation'] * (1 + 0.3 * summer_factor)\n",
        "    df['solar_intensity'] = df['solar_intensity'].clip(0, 1.3)\n",
        "\n",
        "    # Peak hours with granularity\n",
        "    df['is_morning_peak'] = ((df['hour'] >= 7) & (df['hour'] <= 9)).astype(int)\n",
        "    df['is_solar_peak'] = ((df['hour'] >= 11) & (df['hour'] <= 14)).astype(int)\n",
        "    df['is_afternoon_peak'] = ((df['hour'] >= 15) & (df['hour'] <= 17)).astype(int)\n",
        "    df['is_evening_peak'] = ((df['hour'] >= 18) & (df['hour'] <= 21)).astype(int)\n",
        "    df['is_night'] = ((df['hour'] <= 5) | (df['hour'] >= 22)).astype(int)\n",
        "\n",
        "    # System features\n",
        "    df['is_residential'] = (df['connection_type'] == 'RESIDENTIAL').astype(int)\n",
        "    df['is_commercial'] = (df['connection_type'] == 'COMMERCIAL').astype(int)\n",
        "\n",
        "    # Capacity features\n",
        "    df['capacity_ratio'] = df['panels_capacity_W'] / (df['load_capacity_W'] + 1)\n",
        "    df['log_panels_capacity'] = np.log1p(df['panels_capacity_W'])\n",
        "    df['log_load_capacity'] = np.log1p(df['load_capacity_W'])\n",
        "    df['total_capacity'] = df['panels_capacity_W'] + df['load_capacity_W']\n",
        "\n",
        "    # Interaction features\n",
        "    df['solar_x_capacity'] = df['solar_elevation'] * df['panels_capacity_W']\n",
        "    df['hour_x_residential'] = df['hour'] * df['is_residential']\n",
        "    df['hour_x_commercial'] = df['hour'] * df['is_commercial']\n",
        "    df['weekend_x_residential'] = df['day_of_week'].isin([5, 6]).astype(int) * df['is_residential']\n",
        "\n",
        "    # FIX: Location encoding with consistent features\n",
        "    if top_locations is not None:\n",
        "        # Use provided top locations (from training data)\n",
        "        for loc in top_locations:\n",
        "            df[f'loc_{loc}'] = (df['location'] == loc).astype(int)\n",
        "    else:\n",
        "        # This is training data, determine top locations\n",
        "        top_locations = df['location'].value_counts().head(10).index\n",
        "        for loc in top_locations:\n",
        "            df[f'loc_{loc}'] = (df['location'] == loc).astype(int)\n",
        "\n",
        "    # Advanced time features\n",
        "    df['minutes_from_noon'] = np.abs((df['hour'] * 60 + df['minute']) - 720)\n",
        "    df['minutes_from_midnight'] = np.minimum(\n",
        "        df['hour'] * 60 + df['minute'],\n",
        "        1440 - (df['hour'] * 60 + df['minute'])\n",
        "    )\n",
        "\n",
        "    return df, top_locations\n",
        "\n",
        "# REPLACE the feature engineering section with this:\n",
        "\n",
        "print(\"\\nüîß Creating advanced features...\")\n",
        "\n",
        "# Process training data and get top locations\n",
        "train_processed, top_locations_from_train = create_advanced_features(train_df.copy(), systems_df.copy())\n",
        "\n",
        "# Process test data using the same top locations from training\n",
        "test_processed, _ = create_advanced_features(test_df_original.copy(), systems_df.copy(), top_locations=top_locations_from_train)\n",
        "\n",
        "# CRITICAL: Normalize targets - Moved before feature selection\n",
        "train_processed['generation_normalized'] = (\n",
        "    train_processed['generation_W'] / (train_processed['panels_capacity_W'] + 1)\n",
        ").clip(0, 1.5)\n",
        "train_processed['load_normalized'] = (\n",
        "    train_processed['load_W'] / (train_processed['load_capacity_W'] + 1)\n",
        ").clip(0, 2.0)\n",
        "\n",
        "print(f\"‚úÖ Created {len(train_processed.columns)} features\")\n",
        "print(f\"‚úÖ Top locations used: {list(top_locations_from_train)}\")\n",
        "\n",
        "# Now verify features match - Adjusted logic\n",
        "train_cols = set(train_processed.columns)\n",
        "test_cols = set(test_processed.columns)\n",
        "\n",
        "# Identify columns unique to each set (excluding test_id from this check)\n",
        "unique_to_train = train_cols - test_cols - {'generation_normalized', 'load_normalized'}\n",
        "unique_to_test = test_cols - train_cols - {'test_id'}\n",
        "\n",
        "if unique_to_train:\n",
        "    print(f\"‚ö†Ô∏è Columns unique to train: {unique_to_train}\")\n",
        "    # Drop columns unique to train from train_processed (except target variables)\n",
        "    train_processed = train_processed.drop(columns=list(unique_to_train), errors='ignore')\n",
        "\n",
        "if unique_to_test:\n",
        "    print(f\"‚ö†Ô∏è Columns unique to test: {unique_to_test}\")\n",
        "    # Drop columns unique to test from test_processed\n",
        "    test_processed = test_processed.drop(columns=list(unique_to_test), errors='ignore')\n",
        "\n",
        "# Ensure column order is the same for train and test (excluding test_id from test)\n",
        "common_cols = [col for col in train_processed.columns if col in test_processed.columns]\n",
        "train_processed = train_processed[common_cols + ['generation_normalized', 'load_normalized']] # Keep targets in train\n",
        "test_processed = test_processed[common_cols + ['test_id']] # Keep test_id in test\n",
        "\n",
        "# Add a check to ensure feature columns match after dropping\n",
        "feature_cols_train_check = set(train_processed.columns) - {'generation_normalized', 'load_normalized'}\n",
        "feature_cols_test_check = set(test_processed.columns) - {'test_id'}\n",
        "\n",
        "if feature_cols_train_check != feature_cols_test_check:\n",
        "     print(\"‚ùå Error: Feature columns in train_processed and test_processed do not match after dropping!\")\n",
        "     print(\"Features in train_processed:\", sorted(list(feature_cols_train_check)))\n",
        "     print(\"Features in test_processed:\", sorted(list(feature_cols_test_check)))\n",
        "else:\n",
        "     print(\"‚úÖ Feature columns in train_processed and test_processed match.\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 3. FEATURE SELECTION\n",
        "# ============================================\n",
        "\n",
        "# Select features (exclude identifiers and targets)\n",
        "exclude_cols = ['timestamp', 'system_id', 'generation_W', 'load_W',\n",
        "                'connection_type',\n",
        "                'location', 'test_id', 'generation_normalized', 'load_normalized'] # Explicitly exclude normalized targets\n",
        "feature_cols = [col for col in train_processed.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"üìä Using {len(feature_cols)} features for modeling\")\n",
        "\n",
        "X_train = train_processed[feature_cols]\n",
        "y_train_gen = train_processed['generation_normalized']\n",
        "y_train_load = train_processed['load_normalized']\n",
        "X_test = test_processed[feature_cols]"
      ],
      "metadata": {
        "id": "-FhUnWi1fsi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING"
      ],
      "metadata": {
        "id": "T7VYZL35f12r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 4. LIGHTGBM WITH HYPERPARAMETER TUNING\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPTIMIZED LIGHTGBM MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "def train_lgb_with_cv(X, y, params=None, n_folds=5):\n",
        "    \"\"\"Train LightGBM with cross-validation\"\"\"\n",
        "\n",
        "    if params is None:\n",
        "        params = {\n",
        "            'objective': 'regression',\n",
        "            'metric': 'mae',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'num_leaves': 50,\n",
        "            'learning_rate': 0.02,\n",
        "            'feature_fraction': 0.7,\n",
        "            'bagging_fraction': 0.7,\n",
        "            'bagging_freq': 5,\n",
        "            'min_child_samples': 20,\n",
        "            'reg_alpha': 0.1,\n",
        "            'reg_lambda': 0.1,\n",
        "            'verbose': -1,\n",
        "            'num_threads': -1\n",
        "        }\n",
        "\n",
        "    # Cross-validation for robust training\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    models = []\n",
        "    scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
        "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        train_data = lgb.Dataset(X_fold_train, label=y_fold_train)\n",
        "        val_data = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data)\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            valid_sets=[val_data],\n",
        "            num_boost_round=1000,\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(50),\n",
        "                lgb.log_evaluation(0)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        models.append(model)\n",
        "        val_pred = model.predict(X_fold_val, num_iteration=model.best_iteration)\n",
        "        mae = mean_absolute_error(y_fold_val, val_pred)\n",
        "        scores.append(mae)\n",
        "        print(f\"  Fold {fold} MAE: {mae:.5f}\")\n",
        "\n",
        "    print(f\"  Average MAE: {np.mean(scores):.5f} (¬±{np.std(scores):.5f})\")\n",
        "    return models\n",
        "\n",
        "# Train generation models\n",
        "print(\"\\nüìä Training LightGBM for generation...\")\n",
        "lgb_gen_models = train_lgb_with_cv(X_train, y_train_gen)\n",
        "\n",
        "# Train load models\n",
        "print(\"\\nüìä Training LightGBM for load...\")\n",
        "lgb_load_models = train_lgb_with_cv(X_train, y_train_load)\n",
        "\n",
        "# ============================================\n",
        "# 5. XGBOOST MODELS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"XGBOOST MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "\n",
        "    def train_xgb_with_cv(X, y, n_folds=5):\n",
        "        \"\"\"Train XGBoost with cross-validation\"\"\"\n",
        "\n",
        "        params = {\n",
        "            'objective': 'reg:absoluteerror',\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.02,\n",
        "            'n_estimators': 1000,\n",
        "            'subsample': 0.7,\n",
        "            'colsample_bytree': 0.7,\n",
        "            'reg_alpha': 0.1,\n",
        "            'reg_lambda': 0.1,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1,\n",
        "            'verbosity': 0\n",
        "        }\n",
        "\n",
        "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        models = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
        "            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "            model = xgb.XGBRegressor(\n",
        "                **params,\n",
        "                eval_metric='mae',\n",
        "                early_stopping_rounds=50\n",
        "            )\n",
        "            model.fit(\n",
        "                X_fold_train, y_fold_train,\n",
        "                eval_set=[(X_fold_val, y_fold_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            models.append(model)\n",
        "            print(f\"  Fold {fold} trained\")\n",
        "\n",
        "        return models\n",
        "\n",
        "    print(\"üìä Training XGBoost for generation...\")\n",
        "    xgb_gen_models = train_xgb_with_cv(X_train, y_train_gen)\n",
        "\n",
        "    print(\"üìä Training XGBoost for load...\")\n",
        "    xgb_load_models = train_xgb_with_cv(X_train, y_train_load)\n",
        "\n",
        "    USE_XGBOOST = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è XGBoost not available\")\n",
        "    USE_XGBOOST = False\n",
        "\n",
        "# ============================================\n",
        "# 6. CATBOOST MODELS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CATBOOST MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "\n",
        "    def train_catboost_with_cv(X, y, n_folds=3):\n",
        "        \"\"\"Train CatBoost with cross-validation\"\"\"\n",
        "\n",
        "        params = {\n",
        "            'loss_function': 'MAE',\n",
        "            'iterations': 1000,\n",
        "            'learning_rate': 0.03,\n",
        "            'depth': 6,\n",
        "            'l2_leaf_reg': 3,\n",
        "            'random_seed': 42,\n",
        "            'verbose': False\n",
        "        }\n",
        "\n",
        "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        models = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
        "            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "            model = cb.CatBoostRegressor(**params)\n",
        "            model.fit(\n",
        "                X_fold_train, y_fold_train,\n",
        "                eval_set=(X_fold_val, y_fold_val),\n",
        "                early_stopping_rounds=50,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            models.append(model)\n",
        "            print(f\"  Fold {fold} trained\")\n",
        "\n",
        "        return models\n",
        "\n",
        "    print(\"üìä Training CatBoost for generation...\")\n",
        "    cb_gen_models = train_catboost_with_cv(X_train, y_train_gen)\n",
        "\n",
        "    print(\"üìä Training CatBoost for load...\")\n",
        "    cb_load_models = train_catboost_with_cv(X_train, y_train_load)\n",
        "\n",
        "    USE_CATBOOST = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è CatBoost not available\")\n",
        "    USE_CATBOOST = False\n",
        "\n",
        "# ============================================\n",
        "# 7. STACKING WITH META-LEARNER\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STACKING ENSEMBLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def create_meta_features(models_dict, X, model_type='lgb'):\n",
        "    \"\"\"Create predictions from base models for meta-learner\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for models in models_dict[model_type]:\n",
        "        if model_type == 'lgb':\n",
        "            pred = models.predict(X, num_iteration=models.best_iteration)\n",
        "        else:\n",
        "            pred = models.predict(X)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    # Return mean and std of predictions\n",
        "    preds_array = np.array(predictions)\n",
        "    return np.mean(preds_array, axis=0), np.std(preds_array, axis=0)\n",
        "\n",
        "# Create meta features for training\n",
        "print(\"Creating meta features...\")\n",
        "meta_train_gen = []\n",
        "meta_train_load = []\n",
        "\n",
        "# LightGBM predictions\n",
        "lgb_gen_mean, lgb_gen_std = create_meta_features({'lgb': lgb_gen_models}, X_train, 'lgb')\n",
        "lgb_load_mean, lgb_load_std = create_meta_features({'lgb': lgb_load_models}, X_train, 'lgb')\n",
        "meta_train_gen.extend([lgb_gen_mean, lgb_gen_std])\n",
        "meta_train_load.extend([lgb_load_mean, lgb_load_std])\n",
        "\n",
        "if USE_XGBOOST:\n",
        "    xgb_gen_mean, xgb_gen_std = create_meta_features({'xgb': xgb_gen_models}, X_train, 'xgb')\n",
        "    xgb_load_mean, xgb_load_std = create_meta_features({'xgb': xgb_load_models}, X_train, 'xgb')\n",
        "    meta_train_gen.extend([xgb_gen_mean, xgb_gen_std])\n",
        "    meta_train_load.extend([xgb_load_mean, xgb_load_std])\n",
        "\n",
        "if USE_CATBOOST:\n",
        "    cb_gen_mean, cb_gen_std = create_meta_features({'cb': cb_gen_models}, X_train, 'cb')\n",
        "    cb_load_mean, cb_load_std = create_meta_features({'cb': cb_load_models}, X_train, 'cb')\n",
        "    meta_train_gen.extend([cb_gen_mean, cb_gen_std])\n",
        "    meta_train_load.extend([cb_load_mean, cb_load_std])\n",
        "\n",
        "# Stack meta features\n",
        "X_meta_gen = np.column_stack(meta_train_gen)\n",
        "X_meta_load = np.column_stack(meta_train_load)\n",
        "\n",
        "print(f\"Meta features shape: {X_meta_gen.shape}\")\n",
        "\n",
        "# Train meta-learners (simple linear model to avoid overfitting)\n",
        "print(\"\\nTraining meta-learners...\")\n",
        "meta_model_gen = Ridge(alpha=1.0)\n",
        "meta_model_gen.fit(X_meta_gen, y_train_gen)\n",
        "\n",
        "meta_model_load = Ridge(alpha=1.0)\n",
        "meta_model_load.fit(X_meta_load, y_train_load)\n",
        "\n",
        "print(\"‚úÖ Meta-learners trained\")"
      ],
      "metadata": {
        "id": "GLBH7u8ef7rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREDICTION AND SUBMISSION"
      ],
      "metadata": {
        "id": "_p1uAUr8gMWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 8. GENERATE TEST PREDICTIONS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING TEST PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create meta features for test\n",
        "meta_test_gen = []\n",
        "meta_test_load = []\n",
        "\n",
        "# LightGBM test predictions\n",
        "lgb_test_gen = np.mean([m.predict(X_test, num_iteration=m.best_iteration)\n",
        "                        for m in lgb_gen_models], axis=0)\n",
        "lgb_test_load = np.mean([m.predict(X_test, num_iteration=m.best_iteration)\n",
        "                         for m in lgb_load_models], axis=0)\n",
        "\n",
        "# For stacking\n",
        "lgb_test_gen_std = np.std([m.predict(X_test)\n",
        "                           for m in lgb_gen_models], axis=0)\n",
        "lgb_test_load_std = np.std([m.predict(X_test)\n",
        "                            for m in lgb_load_models], axis=0)\n",
        "\n",
        "meta_test_gen.extend([lgb_test_gen, lgb_test_gen_std])\n",
        "meta_test_load.extend([lgb_test_load, lgb_test_load_std])\n",
        "\n",
        "if USE_XGBOOST:\n",
        "    xgb_test_gen = np.mean([m.predict(X_test) for m in xgb_gen_models], axis=0)\n",
        "    xgb_test_load = np.mean([m.predict(X_test) for m in xgb_load_models], axis=0)\n",
        "    xgb_test_gen_std = np.std([m.predict(X_test) for m in xgb_gen_models], axis=0)\n",
        "    xgb_test_load_std = np.std([m.predict(X_test) for m in xgb_load_models], axis=0)\n",
        "    meta_test_gen.extend([xgb_test_gen, xgb_test_gen_std])\n",
        "    meta_test_load.extend([xgb_test_load, xgb_test_load_std])\n",
        "\n",
        "if USE_CATBOOST:\n",
        "    cb_test_gen = np.mean([m.predict(X_test) for m in cb_gen_models], axis=0)\n",
        "    cb_test_load = np.mean([m.predict(X_test) for m in cb_load_models], axis=0)\n",
        "    cb_test_gen_std = np.std([m.predict(X_test) for m in cb_gen_models], axis=0)\n",
        "    cb_test_load_std = np.std([m.predict(X_test) for m in cb_load_models], axis=0)\n",
        "    meta_test_gen.extend([cb_test_gen, cb_test_gen_std])\n",
        "    meta_test_load.extend([cb_test_load, cb_test_load_std])\n",
        "\n",
        "# Get stacked predictions\n",
        "X_meta_test_gen = np.column_stack(meta_test_gen)\n",
        "X_meta_test_load = np.column_stack(meta_test_load)\n",
        "\n",
        "stacked_gen_pred = meta_model_gen.predict(X_meta_test_gen)\n",
        "stacked_load_pred = meta_model_load.predict(X_meta_test_load)\n",
        "\n",
        "# Also calculate simple average (often works well)\n",
        "if USE_XGBOOST and USE_CATBOOST:\n",
        "    avg_gen_pred = (lgb_test_gen + xgb_test_gen + cb_test_gen) / 3\n",
        "    avg_load_pred = (lgb_test_load + xgb_test_load + cb_test_load) / 3\n",
        "elif USE_XGBOOST:\n",
        "    avg_gen_pred = (lgb_test_gen + xgb_test_gen) / 2\n",
        "    avg_load_pred = (lgb_test_load + xgb_test_load) / 2\n",
        "else:\n",
        "    avg_gen_pred = lgb_test_gen\n",
        "    avg_load_pred = lgb_test_load\n",
        "\n",
        "# Choose best ensemble method (you can experiment)\n",
        "USE_STACKING = True  # Set to False to use simple average\n",
        "\n",
        "if USE_STACKING:\n",
        "    final_gen_normalized = stacked_gen_pred\n",
        "    final_load_normalized = stacked_load_pred\n",
        "    print(\"Using stacked predictions\")\n",
        "else:\n",
        "    final_gen_normalized = avg_gen_pred\n",
        "    final_load_normalized = avg_load_pred\n",
        "    print(\"Using averaged predictions\")\n",
        "\n",
        "# ============================================\n",
        "# 9. DENORMALIZE AND CREATE SUBMISSION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING SUBMISSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Denormalize predictions\n",
        "final_gen_pred = final_gen_normalized * test_processed['panels_capacity_W'].values\n",
        "final_load_pred = final_load_normalized * test_processed['load_capacity_W'].values\n",
        "\n",
        "# Ensure non-negative\n",
        "final_gen_pred = np.maximum(0, final_gen_pred)\n",
        "final_load_pred = np.maximum(0, final_load_pred)\n",
        "\n",
        "print(f\"Generation range: {final_gen_pred.min():.2f} - {final_gen_pred.max():.2f} W\")\n",
        "print(f\"Load range: {final_load_pred.min():.2f} - {final_load_pred.max():.2f} W\")\n",
        "\n",
        "# Create submission\n",
        "submission = test_df_original[['test_id', 'generation_W', 'load_W']].copy()\n",
        "\n",
        "# Replace -1 values\n",
        "mask_gen_predict = (submission['generation_W'] == -1)\n",
        "mask_load_predict = (submission['load_W'] == -1)\n",
        "\n",
        "submission.loc[mask_gen_predict, 'generation_W'] = final_gen_pred[mask_gen_predict]\n",
        "submission.loc[mask_load_predict, 'load_W'] = final_load_pred[mask_load_predict]\n",
        "\n",
        "# Handle -2 values\n",
        "if (sample_submission['generation_W'] < 0).sum() == 0:\n",
        "    submission.loc[submission['generation_W'] == -2, 'generation_W'] = 0\n",
        "    submission.loc[submission['load_W'] == -2, 'load_W'] = 0\n",
        "\n",
        "# Ensure match with sample submission\n",
        "final_submission = sample_submission[['test_id']].merge(\n",
        "    submission, on='test_id', how='left'\n",
        ")\n",
        "\n",
        "for col in ['generation_W', 'load_W']:\n",
        "    if col in sample_submission.columns:\n",
        "        final_submission[col] = final_submission[col].fillna(sample_submission[col])\n",
        "    final_submission[col] = final_submission[col].fillna(0.0).clip(lower=0)\n",
        "\n",
        "# Save\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "submission_filename = f'submission_stacked_{timestamp}.csv'\n",
        "final_submission.to_csv(submission_filename, index=False)\n",
        "final_submission.to_csv('submission_stacked.csv', index=False)\n",
        "\n",
        "print(f\"‚úÖ Submission saved as: {submission_filename}\")"
      ],
      "metadata": {
        "id": "LplROfEXgRRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONLCUSION"
      ],
      "metadata": {
        "id": "ztgdK2nigXxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 10. FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FEATURE IMPORTANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get average feature importance from LightGBM models\n",
        "importance_gen = np.mean([m.feature_importance(importance_type='gain')\n",
        "                          for m in lgb_gen_models], axis=0)\n",
        "importance_load = np.mean([m.feature_importance(importance_type='gain')\n",
        "                           for m in lgb_load_models], axis=0)\n",
        "\n",
        "# Create importance dataframe\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance_gen': importance_gen,\n",
        "    'importance_load': importance_load,\n",
        "    'importance_avg': (importance_gen + importance_load) / 2\n",
        "}).sort_values('importance_avg', ascending=False)\n",
        "\n",
        "print(\"\\nüìä Top 15 Most Important Features:\")\n",
        "print(importance_df.head(15)[['feature', 'importance_avg']])\n",
        "\n",
        "# Save importance for analysis\n",
        "importance_df.to_csv('feature_importance.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ ADVANCED STACKING PIPELINE COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "üìä Models Used:\n",
        "- LightGBM: {len(lgb_gen_models)} folds\n",
        "{\"- XGBoost: \" + str(len(xgb_gen_models)) + \" folds\" if USE_XGBOOST else \"\"}\n",
        "{\"- CatBoost: \" + str(len(cb_gen_models)) + \" folds\" if USE_CATBOOST else \"\"}\n",
        "- Meta-learner: {\"Stacking with Ridge\" if USE_STACKING else \"Simple Average\"}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "x7wOSZfCgVuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}